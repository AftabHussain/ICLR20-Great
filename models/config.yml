ggnn:
  embed_dim: 512
  hidden_dim: 512
  time_steps: [3, 1, 3, 1]
  residuals:
    1: [0]
    3: [0, 1]
  dropout_rate: 0.1
transformer:
  embed_dim: 512
  hidden_dim: 512
  ff_dim: 2048
  num_layers: 6
  attention_dim: 512
  num_heads: 8
  dropout_rate: 0.1
rnn:
  embed_dim: 512
  hidden_dim: 512
  num_layers: 2
  dropout_rate: 0.1
training:
  num_epochs: 100
  print_freq: 25
  learning_rate: 0.0001
  model: "great"
data:
  max_batch_size: 5000
  max_sequence_length: 500
vocabulary:
  subtoken_aggregate: "avg"